<!DOCTYPE html>
<html lang="en">

  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0"/>

    <title>Effective Approaches To Attention Based Neural Machine Translation</title>
    <meta name="description" content="Abstract" />
    
  
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link rel="stylesheet" type="text/css" media="screen,projection" href="/assets/materialize.css" />
    <link rel="stylesheet" type="text/css" media="screen,projection" href="/assets/main.css">
    <link rel="canonical" href="http://localhost:4000/2017/12/01/Effective-Approaches-to-Attention-based-Neural-Machine-Translation.html" />
    <link rel="alternate" type="application/rss+xml" title="Xiaotian Zhao" href="/feed.xml" />
  
    
</head>


  <body>

    <nav class="light-blue lighten-1" role="navigation">
    <div class="nav-wrapper container"><a id="logo-container" class="brand-logo" href="/">Xiaotian Zhao</a>
    
    
          <ul class="right hide-on-med-and-down">
            <li><a href="/">Home </a></li>
            <!-- <li><a herf="/contact/">Contact</li> -->
            
                
                    
                    
                
                    
                    
                    <li><a href="/article.html">Article</a></li>
                    
                
                    
                    
                    <li><a href="/contact.html">Contact</a></li>
                    
                
                    
                    
                
                    
                    
                
                    
                    
                
                          
          </ul>

          <ul id="nav-mobile" class="side-nav">
            <li><a href="#">Home</a></li>
                            
                
                    
                    
                
                    
                    
                    <li><a href="/article.html">Article</a></li>
                    
                
                    
                    
                    <li><a href="/contact.html">Contact</a></li>
                    
                
                    
                    
                
                    
                    
                
                    
                    
                
             
          </ul>        

      <a href="#" data-activates="nav-mobile" class="button-collapse"><i class="material-icons">menu</i></a>
    </div>
</nav>


        <div class="container">
    <div class="section">
    <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">Effective Approaches To Attention Based Neural Machine Translation</h1>
        <p class="post-meta">
          <time datetime="2017-12-01T00:00:00+08:00" itemprop="datePublished">
            
            Dec 1, 2017
          </time>
          </p>
      </header>

      <div class="post-content" itemprop="articleBody">
        <p><strong>Abstract</strong></p>

<p>这篇文章提出了两类有效的Attention机制：每次关注全部源词语的全局方法和一次关注一部分的局部方法．</p>

<p>使用局部的Attention获得了Bleu值5个百分点的提高，跟没有使用Attention的但使用了已知技巧的比如Dropout的方法相比．</p>

<p><strong>1. Introduction</strong></p>

<p>前一个全局的方法很像Bahdanau的方法但是是一个更简单的架构.后一个方法可以看做是一个hard和soft Attention的混合,这个方法计算比全局的方法或者soft Attention计算量更小，和hard Attention相比，局部的Attention是可变的，让训练变的更容易．</p>

<p><strong>2. Neural Machine Translation</strong></p>

<table>
  <tbody>
    <tr>
      <td>神经网络机器翻译是一个神经网络去刻画条件概率p(y</td>
      <td>x)将输入x翻译到输出y.</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>$log p(y</td>
      <td>x) = \sum_{j=1}^m log p(y_j</td>
      <td>y_{&lt;j}, s)$</td>
    </tr>
  </tbody>
</table>

<p>s是输入x经过encoder之后得到的表示</p>

<p>$p(y_j|y_{&lt;j}, s) = softmax(g(h_j))$,
$h_j=f(h_{j-1},s)$</p>

<p><strong>3. Attention Based Model</strong></p>

<table>
  <tbody>
    <tr>
      <td>$h’_t = tanh(W_c[c_t; h_t])$,$p(y_t</td>
      <td>y_{&lt;t}, x) = softmax(W_sh’_t)$</td>
    </tr>
  </tbody>
</table>

<p>c_t是经过Attention对encoder的各个隐层计算的加权向量,h_t是decoder第t步的隐层输出．第一个表达式获得的是一个attentional vector，第二个表达式将这个vector计算之后投射到输出空间上面．</p>

<p><strong>3.1 Global Attention</strong></p>

<p>Dot:$score(h_t,h’_s)=h_th’_s$;</p>

<p>General:$score(h_t,h’_s)=h_t W_a h’_s$;</p>

<p>Concat:$score(h_t,h’_s)=W_a[h_t,h’_s]$;</p>

<p>1.Bahdanau的工作使用的是没有做Stack集单向的decoder;2.计算路径更加简单；3.Bahdanau只使用了concat方法，后面会证明其他的方法比concat方法更好</p>

<p><strong>3.2 Local Attention</strong></p>

<p>soft attention指的是在全部的块上都有概率．而hard attention表示的是每次都attend到被选中的一个块上，hard attention在推断阶段花费不多，但是不可微分需要一些比如变量缩减和增强学习等一些特殊的技巧去训练.</p>

<p>这里采用的local attention机制每次选择一个窗口，所以是可以微分的．</p>

<p>具体的做法，就是模型先生成一个根据时间t获得一个对齐位置pt,由经验选择一个窗口大小D，在[pt-D,pt+D]上做Attention.</p>

<p>Monotonic alignment:假定pt=t</p>

<p>Predictive alignment :根据公式$S · sigmoid(v^⊤_p tanh(W_p h_t))$,$W_p$和$v_p$都是可训练的参数．S是源句子的长度．现在的对齐权重计算加上了一个高斯分布．</p>

<p><strong>3.3 Input-feeding Approach</strong></p>

<p>主要是说，把上一步的$h’_t$和当前的输入进行拼接．</p>

<p>这样做的两方面的作用：1.希望模型能够感知之前的对齐选择；2.我们创建一个在水平方向和垂直方面都可扩展的深度网络．</p>

      </div>

      
    </article>
    </div>
</div>
<script>$('.carousel.carousel-slider').carousel({fullWidth: true});</script>

      <footer class="page-footer orange">
    <div class="container">
      <div class="row">
        <!-- <div class="col l6 s12">
          <h5 class="white-text">Company Bio</h5>
          <p class="grey-text text-lighten-4">We are a team of college students working on this project like it's our full time job. Any amount would help support and continue development on this project and is greatly appreciated.</p>
        </div> -->
        <!-- <div class="col l6 s12">
          <h5 class="white-text">Settings</h5>
          <ul>
            <li><a class="white-text" href="#!">Link 1</a></li>
            <li><a class="white-text" href="#!">Link 2</a></li>
            <li><a class="white-text" href="#!">Link 3</a></li>
            <li><a class="white-text" href="#!">Link 4</a></li>
          </ul>
        </div> -->
        <div class="col l6 s12">
          <h5 class="white-text">Connect</h5>
          <ul>
            <li><a class="white-text" href="https://github.com/xiaotianzhao" target="_blank">xiaotianzhao's github</a></li>
            <li><a class="white-text" href="https://www.weibo.com/u/2270522015">不执__</a></li>
            <!-- <li><a class="white-text" href="#!">Link 3</a></li>
            <li><a class="white-text" href="#!">Link 4</a></li> -->
          </ul>
        </div>
      </div>
    </div>
    <div class="footer-copyright">
        <div class="container">
            <div>
                Template made by <a target="_blank" class="orange-text text-lighten-3" href="http://materializecss.com">Materialize</a>
                <span style="text-align: right; float: right">
                Copyright © 2017 <a href="http://www.panoramedia.it" class="orange-text text-lighten-3">Marco Damiani</a>. Powered by <a target="_blank" href="http://jekyllrb.com/" class="orange-text text-lighten-3">Jekyll</a>
                </span>
            </div>
        </div>
    </div>
</footer>
      
    <!--  Scripts-->                                                                               
<script src="/js/jquery.min.js"></script>
<script src="/js/materialize.min.js"></script>
<script src="/js/init.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/javascript">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
</script>

  </body>

</html>
